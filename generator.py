import utils
import numpy as np
from pathlib import Path
from openai import OpenAI
from typing import List, Tuple, Dict, Callable, Optional


class Generator:
    """
    Generator for producing textual neighbors and counterfactual examples
    using Large Language Models (LLMs) accessed via OpenRouter.
    """

    def __init__(
        self,
        model_name: str,
        api_key: str,
        one_word_neighs_prompt_path: str,
        neighs_prompt_path: str,
        counterfactual_system_prompt_path: str
    ):
        """
        Initializes the Generator and loads system prompts.

        Args:
            model_name (str):
                Name or identifier of the LLM to be used via OpenRouter.
            api_key (str):
                API key for authenticating requests to OpenRouter.
            one_word_neighs_prompt_path (str):
                Path to the system prompt used for one-word neighbor generation.
            neighs_prompt_path (str):
                Path to the system prompt used for iterative neighbor generation.
            counterfactual_system_prompt_path (str):
                Path to the system prompt used for counterfactual generation.

        Returns:
            None
        """
        self.client = OpenAI(
            base_url="https://openrouter.ai/api/v1",
            api_key=api_key
        )

        self.model_name = model_name

        # Load system prompts from disk
        self.one_word_neigh_system_prompt = Path(
            one_word_neighs_prompt_path
        ).read_text()

        self.neighs_system_prompt = Path(
            neighs_prompt_path
        ).read_text()

        self.counterfactual_system_prompt = Path(
            counterfactual_system_prompt_path
        ).read_text()

    def query(
        self,
        model: str,
        system_prompt: str,
        user_prompt: str
    ) -> Tuple[int, int, str]:
        """
        Sends a prompt to the LLM and returns token usage statistics and output.

        Args:
            model (str):
                Name of the LLM model to query.
            system_prompt (str):
                System-level instructions provided to the LLM.
            user_prompt (str):
                User-level input prompt.

        Returns:
            Tuple[int, int, str]:
                - prompt_tokens (int): Number of tokens in the prompt.
                - completion_tokens (int): Number of tokens generated by the model.
                - content (str): Textual response generated by the LLM.
                Returns (0, 0, "") if the query fails.
        """
        try:
            completion = self.client.chat.completions.create(
                model=model,
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": user_prompt}
                ]
            )

            return (
                completion.usage.prompt_tokens,
                completion.usage.completion_tokens,
                completion.choices[0].message.content
            )

        except Exception as e:
            print(f"API Query failed: {e}")
            return 0, 0, ""

    def one_word_neighs_generation(self, sentence: str) -> List[str]:
        """
        Generates sentence variations by removing one word at a time.

        Args:
            sentence (str):
                Input sentence to be perturbed.

        Returns:
            List[str]:
                A list of sentence variations generated by the LLM.
        """
        # Tokenize sentence into individual words
        words = utils.split_sentence(sentence)
        variations = []

        # Generate neighbors for each word independently
        for word in words:
            user_prompt = f'"S" = {sentence}.\n"W" = {word}.'
            _, _, content = self.query(
                self.model_name,
                self.one_word_neigh_system_prompt,
                user_prompt
            )

            # Each line is assumed to be a separate variation
            if content:
                variations.extend(content.splitlines())

        return variations

    def _single_generation(
        self,
        sentence: str,
        label: str,
        system_prompt: str
    ) -> List[str]:
        """
        Generates multiple sentence variations conditioned on a class label.

        This is an internal helper used by neighborhood generation.

        Args:
            sentence (str):
                Input sentence.
            label (str):
                Target class label to condition the generation on.
            system_prompt (str):
                System prompt defining generation behavior.

        Returns:
            List[str]:
                List of generated sentences extracted from the LLM output.
        """
        user_prompt = f'"S" = {sentence}.\n"C" = {label}.'
        _, _, content = self.query(
            self.model_name,
            system_prompt,
            user_prompt
        )

        # Extract only lines following the "$ " delimiter convention
        return [
            line.split('$ ')[1]
            for line in content.splitlines()
            if '$ ' in line
        ]

    def neighbors_generation(
        self,
        sentence: str,
        n: int,
        classes: List[str],
        classifier_fn: Callable,
        max_its: int = 5
    ) -> List[str]:
        """
        Iteratively generates neighboring sentences until a local
        decision boundary is approximated.

        Args:
            sentence (str):
                Input sentence to generate neighbors from.
            n (int):
                Desired number of samples per class.
            classes (List[str]):
                List of class names (assumed binary).
            classifier_fn (Callable):
                Function that maps a list of sentences to class probabilities.
            max_its (int, optional):
                Maximum number of refinement iterations. Defaults to 5.

        Returns:
            List[str]:
                A list containing:
                - n neighbors closest to class 0
                - n neighbors closest to class 1
                - one-word perturbation neighbors
        """
        # Format system prompt with requested number of samples
        sys_prompt = self.neighs_system_prompt.format(n=n)

        # Initial generation for both classes
        sentences = (
            self._single_generation(sentence, classes[0], sys_prompt) +
            self._single_generation(sentence, classes[1], sys_prompt)
        )

        for _ in range(max_its):
            probs = classifier_fn(sentences)
            preds = probs.argmax(axis=1)

            zeros_mask = (preds == 0)
            ones_mask = (preds == 1)

            count_0, count_1 = np.sum(zeros_mask), np.sum(ones_mask)

            # Stop once enough samples from both classes exist
            if count_0 >= n and count_1 >= n:
                break

            # Generate more samples for the underrepresented class
            if count_0 < n:
                best_idx = probs[:, 0].argmax()
                sentences += self._single_generation(
                    sentences[best_idx],
                    classes[0],
                    sys_prompt
                )

            if count_1 < n:
                best_idx = probs[:, 1].argmax()
                sentences += self._single_generation(
                    sentences[best_idx],
                    classes[1],
                    sys_prompt
                )

        # Final validation
        final_probs = classifier_fn(sentences)
        preds = final_probs.argmax(axis=1)

        if np.sum(preds == 0) < n or np.sum(preds == 1) < n:
            raise Exception("Warning: Local boundary not fully found within iteration limits.")

        # Sort sentences by probability of class 1
        sort_idx = final_probs[:, 1].argsort()
        sorted_sentences = np.array(sentences)

        return (
            sorted_sentences[sort_idx[:n]].tolist() +
            sorted_sentences[sort_idx[-n:]].tolist() +
            self.one_word_neighs_generation(sentence)
        )

    def counterfactual_generation(
        self,
        sentence: str,
        explanation: Dict,
        label: int,
        classes: List[str],
        classifier_fn: Callable,
        max_runs: int
    ) -> Optional[Tuple[str, any]]:
        """
        Generates a counterfactual sentence intended to flip
        the classifier's prediction.

        Args:
            sentence (str):
                Original input sentence.
            explanation (Dict):
                Explanation object containing:
                - "tokens": List of feature tokens
                - "coeff": Corresponding feature importance scores
            label (int):
                Original predicted class index.
            classes (List[str]):
                List of class names.
            classifier_fn (Callable):
                Function mapping sentences to class probabilities.
            max_runs (int):
                Maximum number of attempts to generate a valid counterfactual.

        Returns:
            Optional[Tuple[str, any]]:
                - Counterfactual sentence (str)
                - Structured explanation object
                Returns None if no valid counterfactual is found.
        """
        tokens, coeffs = explanation["tokens"], explanation["coeff"]

        # Separate tokens by influence direction
        pos_tokens = [t for t, c in zip(tokens, coeffs) if c > 0]
        neg_tokens = [t for t, c in zip(tokens, coeffs) if c <= 0]

        # token_sets[0] = negative influence tokens
        # token_sets[1] = positive influence tokens
        token_sets = [pos_tokens[::-1], neg_tokens]

        sys_prompt = self.counterfactual_system_prompt.format(
            class0=classes[0],
            class1=classes[1]
        )

        user_prompt = (
            f'"S" = "{sentence}".\n'
            f'"T" = "{classes[label]}".\n'
            f'"F" = "{classes[1 - label]}".\n'
            f'"R" = {token_sets[1 - label]}.\n'
            f'"I" = {token_sets[label]}.'
        )

        for _ in range(max_runs):
            _, _, content = self.query(
                self.model_name,
                sys_prompt,
                user_prompt
            )

            if not content:
                continue

            try:
                cf_sentence, operations = utils.parse_LLM_output(content)

                # Check if prediction flips
                if classifier_fn([cf_sentence])[0].argmax() != label:
                    return (
                        cf_sentence,
                        utils.create_llime_explanation(sentence, operations)
                    )

            except (ValueError, IndexError):
                # Retry if parsing fails
                continue

        return None
